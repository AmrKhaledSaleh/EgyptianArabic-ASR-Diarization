{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-23T13:41:32.009686Z","iopub.status.busy":"2024-07-23T13:41:32.008882Z","iopub.status.idle":"2024-07-23T13:43:30.412793Z","shell.execute_reply":"2024-07-23T13:43:30.411067Z","shell.execute_reply.started":"2024-07-23T13:41:32.009652Z"},"trusted":true},"outputs":[],"source":["! apt-get update && apt-get install -y libsndfile1 ffmpeg\n","! pip install Cython\n","! pip install packaging\n","! pip -q install nemo_toolkit['asr']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T13:43:30.416267Z","iopub.status.busy":"2024-07-23T13:43:30.4157Z","iopub.status.idle":"2024-07-23T13:43:47.523403Z","shell.execute_reply":"2024-07-23T13:43:47.522612Z","shell.execute_reply.started":"2024-07-23T13:43:30.416216Z"},"trusted":true},"outputs":[],"source":["import nemo\n","import nemo.collections.asr as nemo_asr\n","from nemo.collections.asr.models import EncDecCTCModel"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T13:43:47.525158Z","iopub.status.busy":"2024-07-23T13:43:47.524522Z","iopub.status.idle":"2024-07-23T13:43:47.53122Z","shell.execute_reply":"2024-07-23T13:43:47.530211Z","shell.execute_reply.started":"2024-07-23T13:43:47.525108Z"},"trusted":true},"outputs":[],"source":["import os\n","import zipfile\n","import wave\n","import json\n","import pandas as pd\n","import torch\n","from omegaconf import OmegaConf\n","from omegaconf import DictConfig\n","from pytorch_lightning.utilities.model_summary import ModelSummary\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from pytorch_lightning.callbacks import Callback\n","import matplotlib.pyplot as plt\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import librosa"]},{"cell_type":"markdown","metadata":{},"source":["# Manifest Creation:\n","For inference we will need a manifest file that contains a description row of each sample for the model to diarize. <br><br>\n","Manifest Row Example:\n","```json\n","{\"audio_filepath\": \"wavs/audio_sample_75.wav\", \"offset\": 0, \"duration\": 11.0, \"label\": \"infer\", \"text\": \"-\", \"rttm_filepath\": null}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Code to create the manifest file\n","\n","import os\n","import json\n","import librosa\n","\n","def create_wav_manifest(folder_path, output_json_path):\n","    with open(output_json_path, 'w', encoding='utf-8') as fout:\n","        # Loop through all files in the folder\n","        for filename in os.listdir(folder_path):\n","\n","            # Check if the file is a .wav file\n","            if filename.lower().endswith('.wav'):\n","\n","                # Create the full file path\n","                file_path = os.path.join(folder_path, filename)\n","                duration = librosa.core.get_duration(filename=file_path)\n","\n","                # Create a dictionary for this file\n","                file_entry = {\n","                    \"audio_filepath\": file_path,\n","                    \"offset\": 0,\n","                    \"duration\": duration,\n","                    \"label\": \"infer\",\n","                    \"text\": \"-\"\n","                }\n","\n","                json.dump(file_entry, fout)\n","                fout.write('\\n')\n","    \n","    \n","    print(f\"Manifest file created at {output_json_path}\")\n","\n","# Example usage\n","folder_path = 'inference/wavs/'\n","output_json_path = 'inference/test_manifest.json'\n","create_wav_manifest(folder_path, output_json_path)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T14:08:30.419325Z","iopub.status.busy":"2024-07-23T14:08:30.418927Z","iopub.status.idle":"2024-07-23T14:08:30.437629Z","shell.execute_reply":"2024-07-23T14:08:30.436681Z","shell.execute_reply.started":"2024-07-23T14:08:30.419296Z"},"trusted":true},"outputs":[],"source":["inf_config = {\n","    \"name\": \"ClusterDiarizer\",\n","    \"num_workers\": 1,\n","    \"sample_rate\": 16000,\n","    \"batch_size\": 64,\n","    \"device\": 'cuda',\n","    \"verbose\": True,\n","    \"diarizer\": {\n","        \"manifest_filepath\": \"inference/test_manifest.json\",\n","        \"out_dir\": \"/kaggle/working/\",\n","        \"oracle_vad\": False,\n","        \"collar\": 0.25,\n","        \"ignore_overlap\": True,\n","        \"vad\": {\n","            \"model_path\": \"models/vad_multilingual_marblenet.nemo\",\n","            \"external_vad_manifest\": None,\n","            \"parameters\": {\n","                \"window_length_in_sec\": 0.63,\n","                \"shift_length_in_sec\": 0.01,\n","                \"smoothing\": False,\n","                \"overlap\": 0.5,\n","                \"onset\": 0.8,\n","                \"offset\": 0.6,\n","                \"pad_onset\": -0.05,\n","                \"pad_offset\": 0,\n","                \"min_duration_on\": 0,\n","                \"min_duration_off\": 0.6,\n","                \"filter_speech_first\": True\n","            }\n","        },\n","        \"speaker_embeddings\": {\n","            \"model_path\": \"models/titanet-l.nemo\",\n","            \"parameters\": {\n","                \"window_length_in_sec\": [1.5,1.25,1.0,0.75,0.5],\n","                \"shift_length_in_sec\": [0.75,0.625,0.5,0.375,0.1],\n","                \"multiscale_weights\": [1,1,1,1,1],\n","                \"save_embeddings\": True\n","            }\n","        },\n","        \"clustering\": {\n","            \"parameters\": {\n","                \"oracle_num_speakers\": False,\n","                \"max_num_speakers\": 5,\n","                \"enhanced_count_thres\": 80,\n","                \"max_rp_threshold\": 0.25,\n","                \"sparse_search_volume\": 30,\n","                \"maj_vote_spk_count\": False,\n","                \"chunk_cluster_count\": 50,\n","                \"embeddings_per_chunk\": 10000\n","            }\n","        },\n","        \"msdd_model\": {\n","            \"model_path\": 'models/diar_msdd_telephonic.nemo',\n","            \"parameters\": {\n","                \"use_speaker_model_from_ckpt\": True,\n","                \"infer_batch_size\": 25,\n","                \"sigmoid_threshold\": [0.7, 1.0],\n","                \"seq_eval_mode\": False,\n","                \"split_infer\": True,\n","                \"diar_window_length\": 50,\n","                \"overlap_infer_spk_limit\": 5\n","            }\n","        }\n","    }\n","}\n","\n","inf_config = OmegaConf.create(inf_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T14:08:31.803112Z","iopub.status.busy":"2024-07-23T14:08:31.802258Z","iopub.status.idle":"2024-07-23T14:12:59.080695Z","shell.execute_reply":"2024-07-23T14:12:59.079883Z","shell.execute_reply.started":"2024-07-23T14:08:31.803078Z"},"trusted":true},"outputs":[],"source":["from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n","oracle_vad_msdd_model = NeuralDiarizer(cfg=inf_config)\n","\n","oracle_vad_msdd_model.diarize()"]},{"cell_type":"markdown","metadata":{},"source":["The output will be .rttm files with the diarization segments, We need to apply the speech recognition model the these segments so the final output can look like this:\n","\n","```json\n","[\n","    {\n","        \"start\": 0.05,\n","        \"end\": 1.95,\n","        \"speaker\": 2,\n","        \"text\": \"في هاي لايك\"\n","    },\n","    {\n","        \"start\": 1.95,\n","        \"end\": 3.45,\n","        \"speaker\": 4,\n","        \"text\": \"أنا حبيت أوي\"\n","    },\n","    {\n","        \"start\": 3.45,\n","        \"end\": 5.45,\n","        \"speaker\": 0,\n","        \"text\": \" إن أنا كوفين جزء منها و ده\"\n","    },\n","    {\n","        \"start\": 5.45,\n","        \"end\": 6.05,\n","        \"speaker\": 4,\n","        \"text\": \"و ده اللي بحب\"\n","    },\n","    {\n","        \"start\": 6.05,\n","        \"end\": 7.949999999999999,\n","        \"speaker\": 1,\n","        \"text\": \"شكر عليه والله كمان برضو\"\n","    },\n","    {\n","        \"start\": 7.95,\n","        \"end\": 9.55,\n","        \"speaker\": 3,\n","        \"text\": \"الأبضاء في واتش\"\n","    },\n","    {\n","        \"start\": 10.36,\n","        \"end\": 11.0,\n","        \"speaker\": 4,\n","        \"text\": \"إن إحنا\"\n","    }\n","]\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["conformer_ctc = nemo_asr.models.EncDecCTCModel.restore_from('models/Conformer-CTC-Char_final.nemo')\n","summary = ModelSummary(conformer_ctc)\n","print(summary)\n","\n","conformer_ctc.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def parse_rttm(rttm_file):\n","    segments = []\n","    with open(rttm_file, 'r') as file:\n","        lines = file.readlines()\n","        for line in lines:\n","            parts = line.strip().split()\n","            start_time = float(parts[3])\n","            duration = float(parts[4])\n","            end_time = start_time + duration\n","            speaker = parts[7]\n","            segments.append({\n","                \"start\": start_time,\n","                \"end\": end_time,\n","                \"speaker\": speaker\n","            })\n","    return segments\n","\n","def transcribe_segment(wav_file, start, end):\n","    with wave.open(wav_file, 'rb') as wf:\n","        wf.setpos(int(start * wf.getframerate()))\n","        segment_frames = wf.readframes(int((end - start) * wf.getframerate()))\n","        segment_wav_file = 'temp_segment.wav'\n","        with wave.open(segment_wav_file, 'wb') as segment_wf:\n","            segment_wf.setnchannels(wf.getnchannels())\n","            segment_wf.setsampwidth(wf.getsampwidth())\n","            segment_wf.setframerate(wf.getframerate())\n","            segment_wf.writeframes(segment_frames)\n","        transcription = conformer_ctc.transcribe([segment_wav_file])[0]\n","        os.remove(segment_wav_file)\n","    return transcription\n","\n","def process_rttm_and_transcribe(rttm_file, wav_file, output_json):\n","    segments = parse_rttm(rttm_file)\n","    results = []\n","    for segment in segments:\n","        text = transcribe_segment(wav_file, segment[\"start\"], segment[\"end\"])\n","        result = {\n","            \"start\": segment[\"start\"],\n","            \"end\": segment[\"end\"],\n","            \"speaker\": int(segment[\"speaker\"][-1]),\n","            \"text\": text\n","        }\n","        results.append(result)\n","    with open(output_json, 'w', encoding='utf-8') as json_file:\n","        json.dump(results, json_file, indent=4, ensure_ascii=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def process_folder(wav_folder, rttm_folder, output_folder):\n","    # Ensure output folder exists\n","    os.makedirs(output_folder, exist_ok=True)\n","\n","    # Loop through all files in the WAV folder\n","    for filename in os.listdir(wav_folder):\n","        if filename.lower().endswith('.wav'):\n","            # Construct full file paths\n","            wav_file = os.path.join(wav_folder, filename)\n","            rttm_file = os.path.join(rttm_folder, filename.replace('.wav', '.rttm'))\n","            output_json = os.path.join(output_folder, filename.replace('.wav', '.json'))\n","\n","            # Check if RTTM file exists\n","            if not os.path.exists(rttm_file):\n","                print(f\"RTTM file not found for {filename}. Skipping...\")\n","                continue\n","\n","            # Process the file\n","            try:\n","                process_rttm_and_transcribe(rttm_file, wav_file, output_json)\n","                print(f\"Processed {filename} successfully.\")\n","            except Exception as e:\n","                print(f\"Error processing {filename}: {str(e)}\")\n","\n","# Example usage\n","wav_folder = 'inference/wavs/'\n","rttm_folder = 'pred_rttms/'\n","output_folder = 'Final_Submission/'\n","\n","process_folder(wav_folder, rttm_folder, output_folder)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5383423,"sourceId":8946123,"sourceType":"datasetVersion"},{"datasetId":5397177,"sourceId":8966075,"sourceType":"datasetVersion"},{"datasetId":5421869,"sourceId":9000655,"sourceType":"datasetVersion"},{"datasetId":5422021,"sourceId":9000881,"sourceType":"datasetVersion"},{"datasetId":5433245,"sourceId":9016822,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
