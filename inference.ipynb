{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-23T13:41:32.009686Z","iopub.status.busy":"2024-07-23T13:41:32.008882Z","iopub.status.idle":"2024-07-23T13:43:30.412793Z","shell.execute_reply":"2024-07-23T13:43:30.411067Z","shell.execute_reply.started":"2024-07-23T13:41:32.009652Z"},"trusted":true},"outputs":[],"source":["! apt-get update && apt-get install -y libsndfile1 ffmpeg\n","! pip install Cython\n","! pip install packaging\n","! pip -q install nemo_toolkit['asr']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T13:43:30.416267Z","iopub.status.busy":"2024-07-23T13:43:30.4157Z","iopub.status.idle":"2024-07-23T13:43:47.523403Z","shell.execute_reply":"2024-07-23T13:43:47.522612Z","shell.execute_reply.started":"2024-07-23T13:43:30.416216Z"},"trusted":true},"outputs":[],"source":["import nemo\n","import nemo.collections.asr as nemo_asr\n","from nemo.collections.asr.models import EncDecCTCModel\n","from nemo.collections.asr.models import EncDecDiarLabelModel\n","from nemo.utils.exp_manager import exp_manager"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T13:43:47.525158Z","iopub.status.busy":"2024-07-23T13:43:47.524522Z","iopub.status.idle":"2024-07-23T13:43:47.53122Z","shell.execute_reply":"2024-07-23T13:43:47.530211Z","shell.execute_reply.started":"2024-07-23T13:43:47.525108Z"},"trusted":true},"outputs":[],"source":["import os\n","import zipfile\n","import json\n","import pandas as pd\n","import torch\n","from omegaconf import OmegaConf\n","from omegaconf import DictConfig\n","import librosa"]},{"cell_type":"markdown","metadata":{},"source":["# Manifest Creation:\n","For inference we will need a manifest file that contains a description row of each sample for the model to diarize. <br><br>\n","Manifest Row Example:\n","```json\n","{\"audio_filepath\": \"wavs/audio_sample_75.wav\", \"offset\": 0, \"duration\": 11.0, \"label\": \"infer\", \"text\": \"-\", \"rttm_filepath\": null}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Code to create the manifest file\n","\n","import os\n","import json\n","import librosa\n","\n","def create_wav_manifest(folder_path, output_json_path):\n","    with open(output_json_path, 'w', encoding='utf-8') as fout:\n","        # Loop through all files in the folder\n","        for filename in os.listdir(folder_path):\n","\n","            # Check if the file is a .wav file\n","            if filename.lower().endswith('.wav'):\n","\n","                # Create the full file path\n","                file_path = os.path.join(folder_path, filename)\n","                duration = librosa.core.get_duration(filename=file_path)\n","\n","                # Create a dictionary for this file\n","                file_entry = {\n","                    \"audio_filepath\": file_path,\n","                    \"offset\": 0,\n","                    \"duration\": duration,\n","                    \"label\": \"infer\",\n","                    \"text\": \"-\"\n","                }\n","\n","                json.dump(file_entry, fout)\n","                fout.write('\\n')\n","    \n","    \n","    print(f\"Manifest file created at {output_json_path}\")\n","\n","# Example usage\n","folder_path = 'inference/wavs/'\n","output_json_path = 'inference/test_manifest.json'\n","create_wav_manifest(folder_path, output_json_path)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T14:08:30.419325Z","iopub.status.busy":"2024-07-23T14:08:30.418927Z","iopub.status.idle":"2024-07-23T14:08:30.437629Z","shell.execute_reply":"2024-07-23T14:08:30.436681Z","shell.execute_reply.started":"2024-07-23T14:08:30.419296Z"},"trusted":true},"outputs":[],"source":["inf_config = {\n","    \"name\": \"ClusterDiarizer\",\n","    \"num_workers\": 1,\n","    \"sample_rate\": 16000,\n","    \"batch_size\": 64,\n","    \"device\": 'cuda',\n","    \"verbose\": True,\n","    \"diarizer\": {\n","        \"manifest_filepath\": \"inference/test_manifest.json\",\n","        \"out_dir\": \"/kaggle/working/\",\n","        \"oracle_vad\": False,\n","        \"collar\": 0.25,\n","        \"ignore_overlap\": True,\n","        \"vad\": {\n","            \"model_path\": \"models/vad_multilingual_marblenet.nemo\",\n","            \"external_vad_manifest\": None,\n","            \"parameters\": {\n","                \"window_length_in_sec\": 0.63,\n","                \"shift_length_in_sec\": 0.01,\n","                \"smoothing\": False,\n","                \"overlap\": 0.5,\n","                \"onset\": 0.8,\n","                \"offset\": 0.6,\n","                \"pad_onset\": -0.05,\n","                \"pad_offset\": 0,\n","                \"min_duration_on\": 0,\n","                \"min_duration_off\": 0.6,\n","                \"filter_speech_first\": True\n","            }\n","        },\n","        \"speaker_embeddings\": {\n","            \"model_path\": \"models/titanet-l.nemo\",\n","            \"parameters\": {\n","                \"window_length_in_sec\": [1.5,1.25,1.0,0.75,0.5],\n","                \"shift_length_in_sec\": [0.75,0.625,0.5,0.375,0.1],\n","                \"multiscale_weights\": [1,1,1,1,1],\n","                \"save_embeddings\": True\n","            }\n","        },\n","        \"clustering\": {\n","            \"parameters\": {\n","                \"oracle_num_speakers\": False,\n","                \"max_num_speakers\": 5,\n","                \"enhanced_count_thres\": 80,\n","                \"max_rp_threshold\": 0.25,\n","                \"sparse_search_volume\": 30,\n","                \"maj_vote_spk_count\": False,\n","                \"chunk_cluster_count\": 50,\n","                \"embeddings_per_chunk\": 10000\n","            }\n","        },\n","        \"msdd_model\": {\n","            \"model_path\": 'models/diar_msdd_telephonic.nemo',\n","            \"parameters\": {\n","                \"use_speaker_model_from_ckpt\": True,\n","                \"infer_batch_size\": 25,\n","                \"sigmoid_threshold\": [0.7, 1.0],\n","                \"seq_eval_mode\": False,\n","                \"split_infer\": True,\n","                \"diar_window_length\": 50,\n","                \"overlap_infer_spk_limit\": 5\n","            }\n","        }\n","    }\n","}\n","\n","inf_config = OmegaConf.create(inf_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T14:08:31.803112Z","iopub.status.busy":"2024-07-23T14:08:31.802258Z","iopub.status.idle":"2024-07-23T14:12:59.080695Z","shell.execute_reply":"2024-07-23T14:12:59.079883Z","shell.execute_reply.started":"2024-07-23T14:08:31.803078Z"},"trusted":true},"outputs":[],"source":["from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n","oracle_vad_msdd_model = NeuralDiarizer(cfg=inf_config)\n","\n","oracle_vad_msdd_model.diarize()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5383423,"sourceId":8946123,"sourceType":"datasetVersion"},{"datasetId":5397177,"sourceId":8966075,"sourceType":"datasetVersion"},{"datasetId":5421869,"sourceId":9000655,"sourceType":"datasetVersion"},{"datasetId":5422021,"sourceId":9000881,"sourceType":"datasetVersion"},{"datasetId":5433245,"sourceId":9016822,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
